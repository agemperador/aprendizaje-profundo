{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESTA ES LA CLASE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Recurrentes - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sec2Vec o Sec2Sec\n",
    "\n",
    "Sirven para información secuencial.\n",
    "\n",
    "Tiene \"memoria\" de lo que vio antes: Con una conexión recurrente.\n",
    "\n",
    "En estos casos la entrada es $X_t$ y la salida es $X_{t+1}$\n",
    "\n",
    "\n",
    "SRNN (vanilla) -> Elman\n",
    "\n",
    "### Doble entrada, una que es la $X_t$ y otra que es la anterior capa $h_{t-1}$\n",
    "\n",
    "### La salida es $X_{t+1}$ y $h_{t+1}$\n",
    "\n",
    "#### El dataset es una base de datos de reseñas de IMDB con tags positivos o negativos\n",
    "\n",
    "Hay que armar un vocabulario de palabras y asignarle un indice a cada palabra\n",
    "\n",
    "Algunos análisis suelen tener en cuenta la frecuencia de palabras\n",
    "\n",
    "Se suelen cortar las palabras según su frecuencia, si cortamos articulos a partir de cierta frecuancia\n",
    "\n",
    "O Se puede cortar la cola de frecuencias, las que aparecen 1 o 2 veces no son significativas\n",
    "\n",
    "### Armamos un one_hot de N-1 0 y 1 en la posición $w_i$ donde $w_i$ es la posición correspondiente a la palabra\n",
    "\n",
    "### $X_t$ es el one_hot_vector. Con estas $X_t$ se genera el entendimiento del lenguaje.\n",
    "\n",
    "### Esta red genera una salida $h_f$ que se va a usar en un clasificador para ver si es positiva o negativa\n",
    "\n",
    "![alt text](RNN.png \"Title\")\n",
    "\n",
    "### El ciclo es: \n",
    "\n",
    "\n",
    "### - Meto $X_t$ sumo el $h_{t-1}$ genero un $Y$ \n",
    "### - Lo retropropago con el $X_{t+1}$\n",
    "### - Luego meto el $x_{t+1}$ y el $h_{t}$ para generar el  $Y_{t+1}$ que se compara con el $X_{t+2}$ ETC...\n",
    "### - Una vez que pase toda la secuencia, uso $h_f$ para clasificar si es 1 o 0 y retropropago para TODOS.\n",
    "\n",
    "\n",
    "### Vemos todo como un clasificador y usamos CrossEntropy\n",
    "\n",
    "### Entrenamos los pesos de $X$ a $h$, $h_{t-1}$ a $h_t$, de $h$ a $Y$ y de $h_f$ a output\n",
    "\n",
    "### Este anda mejor con SGD pero los que son mejores funcionan mejor con Minibatch\n",
    "\n",
    "## LSA, Latent Semantic Analysis\n",
    "\n",
    "\n",
    "\n",
    "# Word Embedding\n",
    "\n",
    "## Palabras semanticamente parecidas tienen embedding parecidos\n",
    "\n",
    "## Word2vec :  A partir de una palabra se fija el contexto.\n",
    "## KING - MAN + WOMAN = QUEEN\n",
    "\n",
    "## Los embeddings de torch toma hasta -2 palabras como contexto.\n",
    "\n",
    "### INPUT - EMBEDDING - RNN \n",
    "\n",
    "\n",
    "#### Para RNN uso dropout para que cada neurona tenga features especificas y no que cada una afecte un poquito, para eso va el model.eval\n",
    "\n",
    "# LSTM devuelve (xo , h , c)\n",
    "\n",
    "\n",
    "## Proba entrenar mientras probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red hecha masomenos a mano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reviewer import *\n",
    "from recurrent_a import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = !ls ./data/*labelled.txt\n",
    "\n",
    "reviews = load(f[0])\n",
    "word_list , word_to_ix = vocabulary(reviews)\n",
    "seq = sequence(reviews, word_to_ix)\n",
    "P = len(reviews)\n",
    "N = len(word_to_ix)\n",
    "T = 2\n",
    "\n",
    "#RNN_train(N,T,P, seq)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como hacer tu propio dataset\n",
    "\n",
    "Todo viene del archivo recurrent_b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-66ff08f4949e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/imdb_labelled.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mseqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load' is not defined"
     ]
    }
   ],
   "source": [
    "reviews = load(\"./data/imdb_labelled.txt\")\n",
    "\n",
    "wl, wd = vocabulary(reviews)\n",
    "\n",
    "seqs = sequence(reviews, wd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0,    0,  ...,  161, 1851,  621],\n",
      "        [   0,    0,    0,  ..., 1441, 1613,  621],\n",
      "        [   0,    0,    0,  ..., 1257, 2841,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 1274, 2760,  621],\n",
      "        [   0,    0,    0,  ..., 1057, 2180,  621],\n",
      "        [   0,    0,    0,  ..., 2143, 2086,  621]])\n",
      "tensor([[   0,    0,    0,  ...,  659, 1668,  621],\n",
      "        [   0,    0,    0,  ...,  891,  190,  621],\n",
      "        [   0,    0,    0,  ..., 2521, 2519,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,  968, 2941,  621],\n",
      "        [   0,    0,    0,  ...,  971,  130,  621],\n",
      "        [   0,    0,    0,  ..., 2426,  102,  621]])\n",
      "tensor([[   0,    0,    0,  ..., 1717,  397,  621],\n",
      "        [   0,    0,    0,  ...,  593,  645,  621],\n",
      "        [   0,    0,    0,  ..., 2430, 1057,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,  940, 2987,  621],\n",
      "        [   0,    0,    0,  ..., 1195, 1948,  621],\n",
      "        [   0,    0,    0,  ..., 2758,  241,  621]])\n",
      "tensor([[   0,    0,    0,  ..., 3047,  513,  621],\n",
      "        [   0,    0,    0,  ..., 1484, 3072,  621],\n",
      "        [   0,    0,    0,  ..., 2899, 2425,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 2987, 2023,  621],\n",
      "        [   0,    0,    0,  ...,  880,  190,  621],\n",
      "        [   0,    0,    0,  ..., 2770, 1174,  621]])\n",
      "tensor([[   0,    0,    0,  ..., 2957, 2624,  621],\n",
      "        [   0,    0,    0,  ..., 2937, 1951,  621],\n",
      "        [   0,    0,    0,  ..., 1699, 2770,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 2143, 2630,  621],\n",
      "        [   0,    0,    0,  ..., 3047,   46,  621],\n",
      "        [   0,    0,    0,  ...,  501, 1174,  621]])\n",
      "0 2.141123741865158\n",
      "tensor([[   0,    0,    0,  ...,  782, 1183,  621],\n",
      "        [   0,    0,    0,  ..., 1545,  741,  621],\n",
      "        [   0,    0,    0,  ...,  968, 2941,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 2899, 2425,  621],\n",
      "        [   0,    0,    0,  ...,  165,  282,  621],\n",
      "        [   0,    0,    0,  ..., 1438, 1879,  621]])\n",
      "tensor([[   0,    0,    0,  ..., 1699, 2770,  621],\n",
      "        [   0,    0,    0,  ..., 1528, 2987,  621],\n",
      "        [   0,    0,    0,  ..., 1234, 1820,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,  971,  130,  621],\n",
      "        [   0,    0,    0,  ..., 2980, 2757,  621],\n",
      "        [   0,    0,    0,  ...,  842, 1734,  621]])\n",
      "tensor([[   0,    0,    0,  ...,  500, 2295,  621],\n",
      "        [   0,    0,    0,  ..., 1616, 2899,  621],\n",
      "        [   0,    0,    0,  ..., 2143, 2983,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 1356,  914,  621],\n",
      "        [   0,    0,    0,  ..., 2180,  530,  621],\n",
      "        [   0,    0,    0,  ..., 1294, 1228,  621]])\n",
      "tensor([[   0,    0,    0,  ..., 2524,  334,  621],\n",
      "        [   0,    0,    0,  ..., 2731, 1057,  621],\n",
      "        [   0,    0,    0,  ...,  593,  645,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 1315, 1641,  621],\n",
      "        [   0,    0,    0,  ..., 2408, 2976,  621],\n",
      "        [   0,    0,    0,  ..., 1457, 2960,  621]])\n",
      "tensor([[   0,    0,    0,  ..., 1995, 2770,  621],\n",
      "        [   0,    0,    0,  ..., 1556,  922,  621],\n",
      "        [   0,    0,    0,  ..., 2286, 1101,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 1839,  674,  621],\n",
      "        [   0,    0,    0,  ...,  940, 2144,  621],\n",
      "        [   0,    0,    0,  ..., 2976,  239,  621]])\n",
      "1 1.701139509677887\n",
      "tensor([[   0,    0,    0,  ..., 2758, 1168,  621],\n",
      "        [   0,    0,    0,  ...,  593,  645,  621],\n",
      "        [   0,    0,    0,  ..., 2548,  959,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 1995, 2770,  621],\n",
      "        [   0,    0,    0,  ...,  209, 1373,  621],\n",
      "        [   0,    0,    0,  ..., 2143, 1382,  621]])\n",
      "tensor([[   0,    0,    0,  ..., 1364,  632,  621],\n",
      "        [   0,    0,    0,  ..., 1967, 2593,  621],\n",
      "        [   0,    0,    0,  ..., 2143, 2180,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,  446,  591,  621],\n",
      "        [   0,    0,    0,  ..., 1930,   42,  621],\n",
      "        [   0,    0,    0,  ..., 1373, 2247,  621]])\n",
      "tensor([[   0,    0,    0,  ..., 1357,  749,  621],\n",
      "        [   0,    0,    0,  ..., 1057,  591,  621],\n",
      "        [   0,    0,    0,  ..., 1866, 1079,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 1438, 2987,  621],\n",
      "        [   0,    0,    0,  ..., 2143, 2983,  621],\n",
      "        [   0,    0,    0,  ..., 1057, 1373,  621]])\n",
      "tensor([[   0,    0,    0,  ..., 2408, 2976,  621],\n",
      "        [   0,    0,    0,  ...,  842, 1734,  621],\n",
      "        [   0,    0,    0,  ...,  842, 2686,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,  754, 1361,  621],\n",
      "        [   0,    0,    0,  ...,  540, 2365,  621],\n",
      "        [   0,    0,    0,  ..., 2192,  515,  621]])\n",
      "tensor([[   0,    0,    0,  ...,  782, 1183,  621],\n",
      "        [   0,    0,    0,  ..., 2921, 1096,  621],\n",
      "        [   0,    0,    0,  ..., 2731, 1057,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 1438, 1623,  621],\n",
      "        [   0,    0,    0,  ..., 2891, 3064,  621],\n",
      "        [   0,    0,    0,  ..., 1577, 1443,  621]])\n",
      "2 1.0578647702932358\n",
      "tensor([[   0,    0,    0,  ...,  782, 1183,  621],\n",
      "        [   0,    0,    0,  ..., 2937, 1951,  621],\n",
      "        [   0,    0,    0,  ...,  971, 1839,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 2524, 1349,  621],\n",
      "        [   0,    0,    0,  ...,  754, 1361,  621],\n",
      "        [   0,    0,    0,  ...,  842,    9,  621]])\n",
      "tensor([[   0,    0,    0,  ..., 2143, 1382,  621],\n",
      "        [   0,    0,    0,  ..., 2899, 2425,  621],\n",
      "        [   0,    0,    0,  ..., 1963, 1318,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,   21,  892,  621],\n",
      "        [   0,    0,    0,  ..., 2686,   70,  621],\n",
      "        [   0,    0,    0,  ..., 1195, 1948,  621]])\n",
      "tensor([[   0,    0,    0,  ..., 2481, 1373,  621],\n",
      "        [   0,    0,    0,  ...,  190, 1364,  621],\n",
      "        [   0,    0,    0,  ...,  946, 2292,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 2987, 2550,  621],\n",
      "        [   0,    0,    0,  ..., 1294, 1587,  621],\n",
      "        [   0,    0,    0,  ..., 1614, 3047,  621]])\n",
      "tensor([[   0,    0,    0,  ..., 1364,  190,  621],\n",
      "        [   0,    0,    0,  ...,  591, 2650,  621],\n",
      "        [   0,    0,    0,  ...,  842, 1734,  621],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 1364,   65,  621],\n",
      "        [   0,    0,    0,  ...,  516, 2899,  621],\n",
      "        [   0,    0,    0,  ..., 1417,  190,  621]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8885ab4553d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlostf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "reviews = load( \"./data/imdb_labelled.txt\")\n",
    "wl, wd = vocabulary( reviews)\n",
    "seqs = sequence( reviews, wd)\n",
    "\n",
    "P = len(reviews)\n",
    "N = len(wl)\n",
    "T = 10\n",
    "B = 50\n",
    "L = 75      # Review Max. Len.\n",
    "\n",
    "#Necesita que la matriz P x L sea long int\n",
    "pads = torch.LongTensor( P, L)\n",
    "for i in range(P):\n",
    "    # Setea todo en 0\n",
    "    seq = seqs[i][0]\n",
    "    M = len(seq)\n",
    "    pads[i] = 0\n",
    "    if M<L:\n",
    "        pads[i,-M:] = torch.tensor(seq)\n",
    "    else:\n",
    "        pads[i] = torch.tensor(seq[:L])\n",
    "\n",
    "        \n",
    "# Guarda los labels\n",
    "labels = torch.tensor([ label for words,label in reviews], dtype=torch.float)\n",
    "\n",
    "# DEJO EL 25% PARA EL TEST\n",
    "trn_inputs =   pads[:-N//4]\n",
    "trn_target = labels[:-N//4]\n",
    "\n",
    "tst_inputs =   pads[-N//4:]\n",
    "tst_target = labels[-N//4:]\n",
    "\n",
    "## Esta linea la dejo asi, funciones virtuales...\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "trn_data = TensorDataset( trn_inputs, trn_target)\n",
    "tst_data = TensorDataset( tst_inputs, tst_target)\n",
    "\n",
    "trn_load = DataLoader( trn_data, shuffle=True, batch_size=B)\n",
    "tst_load = DataLoader( tst_data, shuffle=True, batch_size=B)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Sentiment( torch.nn.Module):\n",
    "    def __init__( _, vocab, embed, context, output=1):\n",
    "        super().__init__()\n",
    "        _.isize = vocab\n",
    "        _.esize = embed\n",
    "        _.hsize = context\n",
    "        _.osize = output\n",
    "        \n",
    "        _.embedding =  torch.nn.Embedding(_.isize, _.esize) # \n",
    "        \n",
    "        # num_layers, dropout apaga neuronas al azar, bidirectional = True entrena otra red al mismo tiempo con las flechas al reves\n",
    "        # esize es el tamaño del embedding y hsize es la salida\n",
    "        _.rnn = torch.nn.GRU(_.esize, _.hsize, batch_first = True) # Cantidad de capas que quiero adentro\n",
    "        \n",
    "        \n",
    "        _.l = torch.nn.Linear(_.hsize,_.osize)\n",
    "\n",
    "    def forward( _, xi):\n",
    "        \n",
    "        print(xi)\n",
    "        \n",
    "        xe = _.embedding(xi)\n",
    "        \n",
    "        # xo son todos los h por los que paso la red y h es el ultimo h de salida\n",
    "        xo, h = _.rnn(xe)\n",
    "        \n",
    "        y = torch.tanh(_.l(h))\n",
    "        \n",
    "        # squeeze saca todas las dimensiones extras y lo deja como vector\n",
    "        return y.squeeze()\n",
    "\n",
    "\n",
    "model = Sentiment( N, 200, 200, 1)\n",
    "optim = torch.optim.Adam( model.parameters())\n",
    "lostf = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "model.train()\n",
    "err = []\n",
    "for t in range(T):\n",
    "    E = 0.\n",
    "    \n",
    "    for words, label in trn_load:\n",
    "\n",
    "        optim.zero_grad() \n",
    "        \n",
    "        y = model(words)\n",
    "        loss = lostf(y,label)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        err.append(loss.item())\n",
    "\n",
    "\n",
    "        optim.step()\n",
    "        \n",
    "        E += loss.item()\n",
    "        \n",
    "    print( t, E)\n",
    "plt.plot(err)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    r, t = 0, 0\n",
    "    for words, label in tst_load:\n",
    "        senti = model(words)\n",
    "        r += ( (senti>=0.5)==(label==1) ).sum().item()\n",
    "        t += len(label)\n",
    "print( \"Accuracy:\", 100*r/t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
